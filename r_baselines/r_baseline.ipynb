{"cells":[{"cell_type":"markdown","metadata":{"id":"trPIhbRD19Xm"},"source":["# setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2xgbAk019Xn"},"outputs":[],"source":["from copulae.input import generate_copula_net_input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTA_tEcI19Xo"},"outputs":[],"source":["import jax.numpy as jnp\n","import jax.scipy.stats as jss\n","import jax\n","\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqCtDUJH19Xo"},"outputs":[],"source":["import pandas as pd\n","import scipy\n","from scipy.stats import bootstrap\n","import copy\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()"]},{"cell_type":"markdown","metadata":{"id":"7coLlZa419Xp"},"source":["## utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xAmBGJqJ19Xp"},"outputs":[],"source":["def add_train_random_noise(data, num_adds):\n","  new_data = np.random.rand(num_adds, data.shape[1])\n","  return np.concatenate((data, new_data), axis = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9ySaw9L19Xp"},"outputs":[],"source":["def rank_normalization(X):\n","  X = copy.deepcopy(X)\n","  for z in X:\n","      ndata = z.shape[0]\n","      gap = 1./(ndata+1)\n","      nfeats = z.shape[1]\n","      for i in range(nfeats):\n","          z[:, i] = scipy.stats.rankdata(z[:, i], 'ordinal')*gap\n","  return X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCdmvkMK19Xq"},"outputs":[],"source":["def get_set(D_val, data_points):\n","  points = D_val\n","  points = jnp.expand_dims(points, axis=0)\n","\n","  # PDF and CDF for X\n","  kde_x = jss.gaussian_kde(data_points[0], bw_method='silverman')\n","  density_x = kde_x.evaluate(points[0, 0, :])\n","  cumulative_x = jnp.array([kde_x.integrate_box_1d(-jnp.inf, p) for p in points[0, 0, :]])\n","\n","  # PDF and CDF for Y\n","  kde_y = jss.gaussian_kde(D[1], bw_method='silverman')\n","  density_y = kde_y.evaluate(points[0, 1, :])\n","  cumulative_y = jnp.array([kde_y.integrate_box_1d(-jnp.inf, p) for p in points[0, 1, :]])\n","\n","  I_pdf = density_x.T * density_y.T\n","  I_pdf = jnp.expand_dims(I_pdf, axis=0)\n","  cdf_xy = jnp.array((cumulative_x, cumulative_y))\n","  cdf_xy = jnp.expand_dims(cdf_xy, axis=0)\n","\n","  del density_x\n","  del density_y\n","  del cumulative_x\n","  del cumulative_y\n","\n","  return points, I_pdf, cdf_xy"]},{"cell_type":"markdown","metadata":{"id":"Ncy81Ug-19Xq"},"source":["## real data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zEgl76kr19Xr","outputId":"9f11db3f-275e-45d4-8d6f-c21dbcd02a64"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'gen-AC' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/yutingng/gen-AC.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iCyl5oU19Xr","outputId":"b08bc7f3-6f9b-44d9-d852-5c5e776ca673"},"outputs":[{"name":"stderr","output_type":"stream","text":["<>:5: SyntaxWarning: invalid escape sequence '\\s'\n","<>:5: SyntaxWarning: invalid escape sequence '\\s'\n","/tmp/ipykernel_5989/1178992437.py:5: SyntaxWarning: invalid escape sequence '\\s'\n","  raw_df = pd.read_csv(data_url, sep = \"\\s+\", skiprows = 22, header = None)\n"]}],"source":["class Boston():\n","  def __init__(self):\n","    # read\n","    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n","    raw_df = pd.read_csv(data_url, sep = \"\\s+\", skiprows = 22, header = None)\n","    X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n","    y = raw_df.values[1::2, 2]\n","\n","    # split\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, random_state = 142857)\n","    X_train = np.concatenate((X_train, y_train[:, None]), axis = 1)\n","    X_test  = np.concatenate((X_test, y_test[:, None]), axis = 1)\n","\n","    # norm\n","    [X_train, X_test] = rank_normalization([X_train, X_test])\n","\n","    # noise\n","    X_train = add_train_random_noise(X_train, int(X_train.shape[0]*0.01))\n","\n","    # 2d\n","    train_data = X_train[:, [0, 13]]\n","    test_data = X_test[:, [0, 13]]\n","\n","    # flip\n","    train_data[:, 0] = 1 - train_data[:, 0]\n","    test_data[:, 0] = 1 - test_data[:, 0]\n","\n","    self.train_y = train_data[:, 1].reshape(-1, 1)\n","    self.train_x = train_data[:, 0].reshape(-1, 1)\n","    self.validation_y = test_data[:, 1].reshape(-1, 1)\n","    self.validation_x = test_data[:, 0].reshape(-1, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKrj2OlL19Xs"},"outputs":[],"source":["class INTC_MSFT():\n","  def __init__(self):\n","    # read\n","    intel_f = open('gen-AC/data/raw/INTC_MSFT_GE/INTEL.data', 'r')\n","    intel = np.array(list(map(float, intel_f.readlines())))\n","\n","    ms_f = open('gen-AC/data/raw/INTC_MSFT_GE/MS.data', 'r')\n","    ms = np.array(list(map(float, ms_f.readlines())))\n","\n","    ge_f = open('gen-AC/data/raw/INTC_MSFT_GE/GE.data', 'r')\n","    ge = np.array(list(map(float, ge_f.readlines())))\n","\n","    # split\n","    X = np.concatenate((intel[:, None], ms[:, None]), axis = 1)\n","    X_train, X_test, _, _ = train_test_split(X, X, shuffle = True, random_state = 142857)\n","\n","    # norm\n","    [X_train, X_test] = rank_normalization([X_train, X_test])\n","\n","    # 2d, noise\n","    train_data = X_train[:, [0, 1]]\n","    train_data = add_train_random_noise(train_data, int(train_data.shape[0]*0.01))\n","    test_data = X_test[:, [0, 1]]\n","\n","    self.train_y = train_data[:, 1].reshape(-1, 1)\n","    self.train_x = train_data[:, 0].reshape(-1, 1)\n","    self.validation_y = test_data[:, 1].reshape(-1, 1)\n","    self.validation_x = test_data[:, 0].reshape(-1, 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Tfxu_xg19Xs"},"outputs":[],"source":["class GOOG_FB():\n","  def __init__(self):\n","    # read\n","    goog_f = open('gen-AC/data/raw/FB_GOOG/goog/close.vals', 'r')\n","    goog = np.array(list(map(float, goog_f.readlines())))\n","\n","    fb_f = open('gen-AC/data/raw/FB_GOOG/fb/close.vals', 'r')\n","    fb = np.array(list(map(float, fb_f.readlines())))\n","\n","    # split\n","    X = np.concatenate((goog[:, None], fb[:, None]), axis = 1)\n","    X_train, X_test, _, _ = train_test_split(X, X, shuffle=True, random_state=142857)\n","\n","    # norm\n","    [X_train, X_test] = rank_normalization([X_train, X_test])\n","\n","    # 2d, noise\n","    train_data = X_train[:, [0, 1]]\n","    train_data = add_train_random_noise(train_data, int(train_data.shape[0]*0.01))\n","    test_data = X_test[:, [0, 1]]\n","\n","    self.train_y = train_data[:, 1].reshape(-1, 1)\n","    self.train_x = train_data[:, 0].reshape(-1, 1)\n","    self.validation_y = test_data[:, 1].reshape(-1, 1)\n","    self.validation_x = test_data[:, 0].reshape(-1, 1)"]},{"cell_type":"markdown","metadata":{"id":"jJdjazqz19Xt"},"source":["## synthetic data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i492qnEc19Xt"},"outputs":[],"source":["def generate_gaussian(rho, sample_size=2000):\n","  mean = np.zeros(2)\n","  E = np.zeros(shape=(2, 2)) + rho\n","  E[0, 0] = 1\n","  E[1, 1] =1\n","\n","  D = np.random.multivariate_normal(mean=mean, cov=E, size=(sample_size, )).T\n","\n","  # Generating Train and test data\n","  shuf_indexes = np.random.permutation(sample_size)\n","\n","  train_p = 0.75\n","  n_train = int(D.shape[1] * train_p)\n","  n_test = D.shape[1] - n_train\n","\n","  train_D = D[:, shuf_indexes[:n_train]]\n","  test_D = D[:, shuf_indexes[n_train:]]\n","\n","  return train_D, test_D\n","\n","class Gauss():\n","  def __init__(self, rho):\n","    train_D, test_D = generate_gaussian(rho)\n","\n","    train_D = scaler.fit_transform(train_D.T).T\n","    test_D = scaler.fit_transform(test_D.T).T\n","\n","    self.train_y = train_D[1, :].reshape(-1, 1)\n","    self.train_x = train_D[0, :].reshape(-1, 1)\n","    self.validation_y = test_D[1, :].reshape(-1, 1)\n","    self.validation_x = test_D[0, :].reshape(-1, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tk_JD1k819Xt"},"outputs":[],"source":["# Marshal and Olkin\n","def clayton_sample(theta):\n","  alpha = 1 / theta\n","  beta = 1\n","  V = np.random.gamma(shape=alpha, scale=beta)\n","  R = np.random.exponential(scale=1, size=2)\n","  t = R / V\n","  U = (1 + t) ** (-1/theta)\n","  return U\n","\n","# Generate Clayton Copula with N(0,1) margins\n","def generate_clayton_sample(theta, sample_size=2000):\n","  X = []\n","  Y = []\n","  for _ in range(sample_size):\n","    U = clayton_sample(theta)\n","    X.append(scipy.stats.norm.ppf(U[0]))\n","    Y.append(scipy.stats.norm.ppf(U[1]))\n","\n","  D = np.concatenate((X, Y)).reshape((2, -1))\n","\n","  # Generating Train and test data\n","  shuf_indexes = np.random.permutation(sample_size)\n","\n","  train_p = 0.75\n","  n_train = int(D.shape[1] * train_p)\n","  n_test = D.shape[1] - n_train\n","\n","  train_D = D[:, shuf_indexes[:n_train]]\n","  test_D = D[:, shuf_indexes[n_train:]]\n","\n","  return train_D, test_D\n","\n","class Clayton():\n","  def __init__(self, theta):\n","    train_D, test_D = generate_clayton_sample(theta)\n","\n","    train_D = scaler.fit_transform(train_D.T).T\n","    test_D = scaler.fit_transform(test_D.T).T\n","\n","    self.train_y = train_D[1, :].reshape(-1, 1)\n","    self.train_x = train_D[0, :].reshape(-1, 1)\n","    self.validation_y = test_D[1, :].reshape(-1, 1)\n","    self.validation_x = test_D[0, :].reshape(-1, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gblwpJR19Xt"},"outputs":[],"source":["# Marshal and Olkin\n","def frank_sample(theta):\n","  p = 1 - np.exp(-theta)\n","  V = scipy.stats.logser.rvs(p)\n","  R = np.random.exponential(scale=1, size=2)\n","  t = R / V\n","  U = -1/theta * np.log( 1 - ( (1 - np.exp(-theta)) * (np.exp(-t)) ) )\n","  return U\n","\n","# Generate Frank Copula with N(0,1) margins\n","def generate_frank_sample(theta, sample_size=2000):\n","  X = []\n","  Y = []\n","  for _ in range(sample_size):\n","    U = frank_sample(theta)\n","    X.append(scipy.stats.norm.ppf(U[0]))\n","    Y.append(scipy.stats.norm.ppf(U[1]))\n","\n","  D = np.concatenate((X, Y)).reshape((2, -1))\n","\n","  # Generating Train and test data\n","  shuf_indexes = np.random.permutation(sample_size)\n","\n","  train_p = 0.75\n","  n_train = int(D.shape[1] * train_p)\n","  n_test = D.shape[1] - n_train\n","\n","  train_D = D[:, shuf_indexes[:n_train]]\n","  test_D = D[:, shuf_indexes[n_train:]]\n","\n","  return train_D, test_D\n","\n","class Frank():\n","  def __init__(self, theta):\n","    train_D, test_D = generate_frank_sample(theta)\n","\n","    train_D = scaler.fit_transform(train_D.T).T\n","    test_D = scaler.fit_transform(test_D.T).T\n","\n","    self.train_y = train_D[1, :].reshape(-1, 1)\n","    self.train_x = train_D[0, :].reshape(-1, 1)\n","    self.validation_y = test_D[1, :].reshape(-1, 1)\n","    self.validation_x = test_D[0, :].reshape(-1, 1)"]},{"cell_type":"markdown","metadata":{"id":"YQ81QGTT19Xt"},"source":["# get ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AS0ckeEa19Xu","outputId":"10b9a8f6-80dd-4de2-aeff-95e1c327b45b"},"outputs":[{"name":"stderr","output_type":"stream","text":["An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"]}],"source":["np.random.seed(30091985)\n","key = jax.random.PRNGKey(30091985)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8s05Yed319Xu"},"outputs":[],"source":["# data_loader = Boston()\n","# ds = 'boston'\n","# data_loader = INTC_MSFT()\n","# ds = 'intcmsft'\n","# data_loader = GOOG_FB()\n","# ds = 'googfb'\n","\n","# ds = 'gauss1'\n","# data_loader = Gauss(rho = 0.1)\n","# ds = 'gauss5'\n","# data_loader = Gauss(rho = 0.5)\n","# ds = 'gauss9'\n","# data_loader = Gauss(rho = 0.9)\n","# ds = 'clayton1'\n","# data_loader = Clayton(theta = 1)\n","# ds = 'clayton5'\n","# data_loader = Clayton(theta = 5)\n","# ds = 'clayton10'\n","# data_loader = Clayton(theta = 10)\n","# ds = 'frank1'\n","# data_loader = Frank(theta = 1)\n","# ds = 'frank5'\n","# data_loader = Frank(theta = 5)\n","ds = 'frank10'\n","data_loader = Frank(theta = 10)\n","\n","D = np.array([data_loader.train_x, data_loader.train_y])[:, :, 0]\n","D_val = np.array([data_loader.validation_x, data_loader.validation_y])[:, :, 0]\n","\n","TrainingTensors = generate_copula_net_input(\n","    D=D,\n","    bootstrap=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BinYD9rL19Xu"},"outputs":[],"source":["# _, _, cdf_xy_trn = get_set(D, TrainingTensors.X_batches[0])\n","# np.savetxt('data/{}/trn.csv'.format(ds), cdf_xy_trn[0, :, :].T, delimiter = ',')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZDHaZ1v19Xu"},"outputs":[],"source":["# _, _, cdf_xy_tst = get_set(D_val, TrainingTensors.X_batches[0])\n","# np.savetxt('data/{}/tst.csv'.format(ds), cdf_xy_tst[0, :, :].T, delimiter = ',')"]},{"cell_type":"markdown","metadata":{"id":"7xy3J-j319Xu"},"source":["# eval baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfZ3ihiY19Xu"},"outputs":[],"source":["baselines = ['par', # VineCopula\n","             'bern', 'T', 'TLL1', 'TLL2', 'TLL2nn', 'MR', 'beta', # kdecopula\n","             'pbern', 'pspl1', 'pspl2'] # penRvine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lA6AuoqP19Xv","outputId":"143c8f2c-71c0-4289-e10b-aa4e9396eb78"},"outputs":[{"name":"stdout","output_type":"stream","text":["-1.2809068 -1.432765046839087 -1.1006814066505055\n","-1.2667893 -1.4141365496082787 -1.0971292123451855\n","-1.2481375 -1.410240108228669 -1.0506701263538587\n","-1.2272695 -1.3952562106877384 -1.0299757733825359\n","-1.2250094 -1.3899939287799046 -1.0175329680341898\n","-1.2551966 -1.4131092662080398 -1.0612081362407617\n","-1.2488296 -1.4038185923599946 -1.0602432681881897\n","-1.2517431 -1.4076652031801824 -1.0646107590062521\n","-1.2657864 -1.417002078811819 -1.088006827871696\n","-1.1772825 -1.3588010272679127 -0.9555101304700729\n","-1.2425157 -1.4062757297373993 -1.0501908768900723\n"]}],"source":["for baseline in baselines:\n","    copula_density = np.genfromtxt('data/{}/{}_yhat.csv'.format(ds, baseline), delimiter = ',')\n","\n","    _, I_pdf, _ = get_set(D_val, TrainingTensors.X_batches[0])\n","    points_density = copula_density * I_pdf\n","\n","    res = bootstrap(yhat, np.mean)\n","    # print(baseline, np.mean(yhat), res.confidence_interval)\n","    print(np.mean(yhat), res.confidence_interval[0], res.confidence_interval[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNoIR8NU19Xv"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"copulae","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}