# -*- coding: utf8 -*-
'''
This module is used to generate inputs for copula
neural networks.

This module make's use of regular numpy and not jax numpy,
thus all operations are in main memory.
'''


from copulae.typing import Sequence
from copulae.typing import Tensor
from copulae.typing import Tuple

from statsmodels.distributions.empirical_distribution \
    import ECDF

import numpy as np


def __nn_cond(
    dim1_key, dim2_key, dim1_vals, dim2_vals, C
):
    k1 = np.searchsorted(dim1_vals, dim1_key)
    k2 = np.searchsorted(dim2_vals, dim2_key)
    return C[k1, k2]


def __populate_conditionals(
    U_batches, C_uv, C_vu, us, vs
):
    C_batches = np.zeros_like(U_batches)
    for batch_i in range(U_batches.shape[0]):
        u = U_batches[batch_i, 0]
        v = U_batches[batch_i, 1]

        c_uv = __nn_cond(u, v, us, vs, C_uv)
        c_vu = __nn_cond(v, u, vs, us, C_vu)

        C_batches[batch_i, 0, :] = c_uv
        C_batches[batch_i, 1, :] = c_vu
    return C_batches


def __create_conditionals(ecdfs, D, dim1, dim2):
    ys = ecdfs[dim1][1]
    C = []
    cond = []
    for x, y_cond in zip(*ecdfs[dim2]):
        data = D[dim1, D[dim2] <= x]
        if data.shape[0] >= 1:
            ecdf_conditinal = ECDF(data, side='right')
            ss = np.searchsorted(ecdf_conditinal.y, ys)
            prob = ecdf_conditinal.y[ss]
        else:
            prob = np.zeros(len(ys), dtype=np.float32)
        C.append(prob)
        cond.append(y_cond)
    C = np.array(C, dtype=np.float32)
    cond = np.array(cond, dtype=np.float32)

    return C, cond


def __init_output(n_batches, n_features, batch_size):
    # U is used for the copula training
    # M are the marginal CDFs
    # X are the dataset values related to M
    # Y is the expected copula output
    U_batches = np.zeros(
        shape=(n_batches, n_features, batch_size),
        dtype=np.float32
    )
    M_batches = np.zeros(
        shape=(n_batches, n_features, batch_size),
        dtype=np.float32
    )
    X_batches = np.zeros(
        shape=(n_batches, n_features, batch_size),
        dtype=np.float32
    )
    Y_batches = np.zeros(
        shape=(n_batches, batch_size, 1),
        dtype=np.float32
    )
    return U_batches, M_batches, X_batches, Y_batches


def __populate(
    D: Tensor,
    bootstrap: bool,
    ecdfs: Sequence[Tuple[Tensor, Tensor]],
    min_val: int,
    max_val: int,
    n_batches: int,
    batch_size: int
) -> Tuple[Tensor, Tensor, Tensor, Tensor]:

    n_features = D.shape[0]
    U_batches, M_batches, X_batches, Y_batches = \
        __init_output(n_batches, n_features, batch_size)

    for batch_i in range(n_batches):
        if bootstrap:
            Ub = np.random.uniform(
                size=(n_features, batch_size),
                low=min_val, high=max_val
            )
        else:
            Ub = np.zeros(
                shape=(n_features, batch_size),
                dtype=np.float32
            )
            for j, xy in enumerate(ecdfs):
                Ub[j] = xy[1]

        mask = True
        for j, xy in enumerate(ecdfs):
            pos = np.searchsorted(
                xy[1], Ub[j]
            )

            vals_m = xy[1][pos]
            M_batches[batch_i, j, :] = vals_m

            vals_x = xy[0][pos]
            X_batches[batch_i, j, :] = vals_x

            lt = np.tile(
                D[j], batch_size
            ).reshape(
                D.shape[1],
                batch_size
            ) <= vals_x
            mask = mask & lt

        Yb = mask.mean(axis=0)
        Yb = Yb.reshape(batch_size, 1)

        U_batches[batch_i, :, :] = Ub
        Y_batches[batch_i, :, :] = Yb

    R_batches = np.random.uniform(
        low=min_val, high=max_val - U_batches,
        size=U_batches.shape
    )

    C_uv, vs = __create_conditionals(ecdfs, D, 0, 1)
    C_vu, us = __create_conditionals(ecdfs, D, 1, 0)

    C_batches = __populate_conditionals(
        U_batches, C_uv, C_vu, us, vs
    )

    # small fix for the -infs from statsmodels ecdf
    d = 1e-6
    X_batches[X_batches == -np.inf] = X_batches.min() - d
    return U_batches, M_batches, C_batches, R_batches, \
        X_batches, Y_batches


def generate_copula_net_input(
    D: Tensor,
    bootstrap: bool = True,
    n_batches: int = 128,
    batch_size: int = 64
) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
    '''
    Creates the input tensors needed to train neural
    copulas. If `bootstrap=True` then random inputs are
    generated by empirically sampling from the CDF of the
    data. If `bootstrap=False`, then a single batch is
    returned.

    See the notes below for differences when
    bootstrapping and when not bootstrapping

    **Bootstrapping (bootstrap=True)**

    The returned tensors will be organized in batches
    (a total of `n_batches`), each batch containing
    `batch_size` elements.

    Batches are created using a bootstrap sampling
    procedure which generates samples based on the
    empirical cumulative distribution function (ecdf).

    The steps to produce the batches are as follows:
    1. For each dimension in the dataset `D`:
        a. Generate `batch_size` random numbers uniformly
           in [0, 1].
        b. For each of the numbers above, sample
           `batch_size` data points for that by
           sampling from the ecdf of the dimension.
        c. Store the uniform value from (a) in U; the
           data points in X; the ecdf of X in M
           (for marginal);
    2. and, finally store the joint ecdf for every
       dimension in Y.

    Steps (1) and (2) generate a single batch.

    **No Bootstrap (bootstrap=False)**

    A single batch is returned with the size of the
    dataset. The arguments returned are the same as when
    bootstrapping.

    The arguments `min_val`, `max_val`, `n_batches` and
    `batch_size` are ignored when not bootstrapping.

    Arguments
    ---------
    D: Tensor
        Our dataset of (`n_dimensions`, `n_samples`)
    min_val: int (defaults to 0)
    max_val: int (defaults to 1)
        Min and max values are used to generate the uniform
        random values used as input to the copula. By
        definition a copula receives only values in [0, 1]
        as input. However, we may sample values out of this
        range in order to train corner cases.
    n_batches: int
        Number of batches to generate
    batch_size: int
        The size of each batch

    Returns
    -------
    Six tensors:

    U_batches: Tensor (n_batches, n_dimensions, batch_size)
        The tensor that serves as input to train neural
        copulas.
    M_batches: Tensor (n_batches, n_dimensions, batch_size)
        Marginal cumulative distribution functions (ecdf)
        for each dimension.
    C_batches: Tensor (n_batches, n_dimensions, batch_size)
        Conditional CDFs of the form P[U <= u | V = v] and
        P[V <= v | U = u], u and v are cdf values for each
        dimension
    R_batches: Tensor (n_batches, n_dimensions, batch_size)
        Random width and heights to create rectangles where
        the left corner is the value on U_batches.
    X_batches: Tensor (n_batches, n_dimensions, batch_size)
        Data points associated with each marginal above.
    Y_batches: Tensor (n_batches, n_dimensions, batch_size)
        The output of the neural copula. A joint cumulative
        distribution estimate of the values in `X_batches`.
    '''

    if len(D.shape) != 2 or D.shape[0] != 2:
        raise ValueError('D must be of shape (2, n)')

    if not bootstrap:
        n_batches = 1.0
        batch_size = D.shape[1]

    ecdfs = []
    ecdf = ECDF(D[0], side='right')
    ecdfs.append((ecdf.x, ecdf.y))
    ecdf = ECDF(D[1], side='right')
    ecdfs.append((ecdf.x, ecdf.y))

    from collections import namedtuple
    TrainingTensors = namedtuple(
        'TrainingTensors',
        ['U_batches', 'M_batches', 'C_batches',
         'R_batches', 'X_batches', 'Y_batches']
    )

    import jax.numpy as jnp
    rv = map(lambda x: jnp.array(x),
             __populate(
                 D, bootstrap, ecdfs, 0, 1.0, n_batches,
                 batch_size)
             )
    return TrainingTensors(*rv)
